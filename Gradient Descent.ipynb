{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liner_model(thetas,X):\n",
    "    result = []\n",
    "    for x in X:\n",
    "        result.append(np.dot(thetas.reshape(len(thetas)),x))#thetas.reshape(len(thetas)保证了将其视为行向量\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_funcion(thetas,X,y): #单次梯度下降\n",
    "    gradient = (1/X.shape[0])*np.dot(X.T,np.dot(X,thetas)-y)#1/X.shape[0]是1/m\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降和线性回归的区别：\n",
    "在上一个代码中，我们会发现thetas是直接通过公式就计算出来了，而在现实中由于数据量、参数特别大，将很难进行计算，并且只使用线性模型\n",
    "因此我们使用了新的方法梯度下降\n",
    "在梯度下降的时候我们的目标函数与线性回归的区别在于我们多了一个1/m，最小二乘法损失函数会与参数规模有关，为了避免这个问题我们加入了1/m来解决这个问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,alpha):\n",
    "    thetas = np.zeros(X.shape[1]).reshape(X.shape[1],1)\n",
    "    gradient = gradient_descent(thetas,X,y)\n",
    "    while not np.all(np.absolute(gradient)<1e-5):\n",
    "        thetas = thetas - alpha*gradient\n",
    "        gradient = gradient_funcion(thetas,X,y)\n",
    "    return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环梯度下降就是希望能找到接近opt的最小点，我们假设当梯度小于1e-5时就是一个最小点\n",
    "因此先设置thetas，使其全为0，然后根据梯度去逐步下降并重新更新梯度，直到梯度小于1e-5时停止并返回thetas，当然我们会发现thetas是全部一起改变的，对于这个问题我们将在后面去讨论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdData = pd.read_csv(\"Salary_Data.csv\", sep=\",\", header=None)\n",
    "    pdData.insert(0,\"x0\",1)#在第一列加入偏执值\n",
    "    orign_data = pdData.values #转换为numpy\n",
    "    cols = orign_data.shape[1] #保存列大小的值\n",
    "\n",
    "    X = orign_data[:,0:cols-1]#要所有行的出了最后一列的值 （一般是真实y）\n",
    "    x = orign_data[:,1:cols-1]\n",
    "    y = orign_data[:,cols-1:cols] #真实y\n",
    "    thetas = gradient_descent(X,y,0.01)\n",
    "\n",
    "    y_predict = liner_model(thetas,X)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "\n",
    "    plt.scatter(pdData.iloc[:,1],pdData.iloc[:,2]) #.iloc基于整数位置索引\n",
    "\n",
    "    plt.xlabel(\"Working age (Years)\")\n",
    "    plt.ylabel(\"Salary (RMB)\")\n",
    "    # 画出线性回归方程的图像\n",
    "    plt.plot(x, y_predict)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
